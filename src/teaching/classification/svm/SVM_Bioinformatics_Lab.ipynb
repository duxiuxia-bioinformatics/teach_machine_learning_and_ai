{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e57abf15",
   "metadata": {},
   "source": [
    "# Support Vector Machines (SVM) Lab — Applied Data Mining for Bioinformatics\n",
    "\n",
    "**Course:** Applied Data Mining for Bioinformatics (Senior UG)\n",
    "\n",
    "**Goal:** Learn how SVMs work (geometrically + practically), and how to train/evaluate them correctly for bioinformatics-style datasets (small *n*, large *p*).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b188f1f",
   "metadata": {},
   "source": [
    "## What you will do\n",
    "1. Build intuition for the **maximum-margin** idea (linear SVM)\n",
    "2. Train a linear SVM on a high-dimensional dataset\n",
    "3. Train an RBF-kernel SVM and tune hyperparameters (**C** and **gamma**)\n",
    "4. Evaluate with confusion matrix, ROC, PR curve\n",
    "5. Learn best practices: scaling, leakage avoidance, and model selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0517c98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, ConfusionMatrixDisplay,\n",
    "    classification_report,\n",
    "    roc_curve, auc,\n",
    "    precision_recall_curve, average_precision_score\n",
    ")\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bb15d4",
   "metadata": {},
   "source": [
    "## Part 1 — Create a bioinformatics-style dataset\n",
    "Bioinformatics often looks like:\n",
    "- **small n** (patients) and **large p** (genes/metabolites)\n",
    "- noisy features\n",
    "- correlated features\n",
    "- mild class imbalance\n",
    "\n",
    "We'll simulate something realistic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05174736",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(\n",
    "    n_samples=120,          # small number of patients\n",
    "    n_features=2000,        # large number of genes/features\n",
    "    n_informative=40,\n",
    "    n_redundant=40,\n",
    "    n_repeated=0,\n",
    "    n_classes=2,\n",
    "    weights=[0.6, 0.4],\n",
    "    class_sep=1.0,\n",
    "    flip_y=0.03,\n",
    "    random_state=7\n",
    ")\n",
    "\n",
    "print('X shape:', X.shape)\n",
    "print('Class balance:', np.bincount(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7f20fd",
   "metadata": {},
   "source": [
    "### Train/test split\n",
    "We will hold out a test set. All hyperparameter tuning must happen on the training set only.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd661f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.25,\n",
    "    stratify=y,\n",
    "    random_state=7\n",
    ")\n",
    "\n",
    "print('Train:', X_train.shape, ' Test:', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd544d93",
   "metadata": {},
   "source": [
    "## Part 2 — Why scaling matters for SVM\n",
    "SVMs are distance-based.\n",
    "\n",
    "**If features are on different scales, the SVM boundary will be dominated by the largest-scale features.**\n",
    "\n",
    "We’ll demonstrate by comparing:\n",
    "- SVM without scaling\n",
    "- SVM with scaling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15747ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "svm_no_scale = SVC(kernel='linear', C=1.0)\n",
    "svm_no_scale.fit(X_train, y_train)\n",
    "pred_no_scale = svm_no_scale.predict(X_test)\n",
    "\n",
    "pipe_scaled = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svm', SVC(kernel='linear', C=1.0))\n",
    "])\n",
    "pipe_scaled.fit(X_train, y_train)\n",
    "pred_scaled = pipe_scaled.predict(X_test)\n",
    "\n",
    "print('Accuracy (no scaling):', accuracy_score(y_test, pred_no_scale))\n",
    "print('Accuracy (scaled):    ', accuracy_score(y_test, pred_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc28dc55",
   "metadata": {},
   "source": [
    "## Part 3 — Linear SVM: margin + support vectors\n",
    "A linear SVM finds a hyperplane that separates classes while maximizing the margin.\n",
    "\n",
    "### Key hyperparameter: **C**\n",
    "- Small C → wider margin, more tolerance for errors (more regularization)\n",
    "- Large C → narrower margin, tries to classify training points correctly (risk of overfitting)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2d423b",
   "metadata": {},
   "source": [
    "### Cross-validation over C (linear SVM)\n",
    "We'll do 5-fold CV on the training set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e85f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "C_grid = [0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=7)\n",
    "\n",
    "scores = []\n",
    "for C in C_grid:\n",
    "    model = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('svm', SVC(kernel='linear', C=C))\n",
    "    ])\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "    scores.append((C, cv_scores.mean(), cv_scores.std()))\n",
    "\n",
    "df_scores = pd.DataFrame(scores, columns=['C', 'mean_acc', 'std_acc'])\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8419e4bc",
   "metadata": {},
   "source": [
    "### Plot CV accuracy vs C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8730fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "plt.errorbar(df_scores['C'], df_scores['mean_acc'], yerr=df_scores['std_acc'], fmt='o-')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('C (log scale)')\n",
    "plt.ylabel('CV accuracy')\n",
    "plt.title('Linear SVM: CV accuracy vs C')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3464a9f6",
   "metadata": {},
   "source": [
    "### Train the best linear SVM and evaluate on test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5327c9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_C = df_scores.loc[df_scores['mean_acc'].idxmax(), 'C']\n",
    "print('Best C:', best_C)\n",
    "\n",
    "best_linear = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svm', SVC(kernel='linear', C=best_C, probability=True))\n",
    "])\n",
    "best_linear.fit(X_train, y_train)\n",
    "\n",
    "y_pred = best_linear.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "ConfusionMatrixDisplay(cm).plot()\n",
    "plt.title('Linear SVM confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4d616c",
   "metadata": {},
   "source": [
    "## Part 4 — ROC curve and PR curve\n",
    "In bioinformatics, **class imbalance is common**.\n",
    "\n",
    "So we often report:\n",
    "- ROC AUC\n",
    "- PR AUC (Average Precision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8eb1a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score = best_linear.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# ROC\n",
    "fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(fpr, tpr)\n",
    "plt.plot([0,1],[0,1],'--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title(f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "plt.show()\n",
    "\n",
    "# PR\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_score)\n",
    "ap = average_precision_score(y_test, y_score)\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(recall, precision)\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title(f'Precision-Recall curve (AP = {ap:.3f})')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a279632",
   "metadata": {},
   "source": [
    "## Part 5 — Nonlinear SVM with RBF kernel\n",
    "When biology is nonlinear (very common), an RBF kernel can work well.\n",
    "\n",
    "### Key hyperparameters\n",
    "- **C**: penalty strength\n",
    "- **gamma**: how local the decision boundary is\n",
    "  - small gamma → smoother boundary\n",
    "  - large gamma → very wiggly boundary (overfits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6266e2",
   "metadata": {},
   "source": [
    "### Grid search for RBF SVM\n",
    "We'll tune (C, gamma) with 5-fold CV on the training set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb06121",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'svm__C': [0.1, 1, 10, 100],\n",
    "    'svm__gamma': [1e-4, 1e-3, 1e-2, 1e-1]\n",
    "}\n",
    "\n",
    "rbf_pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svm', SVC(kernel='rbf', probability=True))\n",
    "])\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    rbf_pipe,\n",
    "    param_grid=param_grid,\n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=7),\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "print('Best params:', grid.best_params_)\n",
    "print('Best CV accuracy:', grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66649ad0",
   "metadata": {},
   "source": [
    "### Evaluate best RBF SVM on test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e62525",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rbf = grid.best_estimator_\n",
    "y_pred_rbf = best_rbf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_rbf))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_rbf)\n",
    "ConfusionMatrixDisplay(cm).plot()\n",
    "plt.title('RBF SVM confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360235bb",
   "metadata": {},
   "source": [
    "## Part 6 — Visual intuition using PCA (2D projection)\n",
    "With 2000 features, we cannot visualize the true hyperplane.\n",
    "\n",
    "But we can project samples into 2D using PCA to see if the classes separate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292d0048",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2, random_state=7)\n",
    "X_train_2d = pca.fit_transform(StandardScaler().fit_transform(X_train))\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.scatter(X_train_2d[:,0], X_train_2d[:,1], c=y_train)\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.title('PCA projection of training data (colored by class)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492630d2",
   "metadata": {},
   "source": [
    "## Part 7 — Common bioinformatics best practices\n",
    "### ✅ Do\n",
    "- Always scale features\n",
    "- Use a pipeline to prevent leakage\n",
    "- Use cross-validation for hyperparameter tuning\n",
    "- Report ROC + PR curves when classes are imbalanced\n",
    "\n",
    "### ❌ Don't\n",
    "- Tune hyperparameters on the test set\n",
    "- Perform feature selection on the full dataset before CV\n",
    "- Interpret SVM coefficients without thinking about scaling and collinearity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f40fd4f",
   "metadata": {},
   "source": [
    "## Optional challenge\n",
    "Try adding feature selection inside the pipeline:\n",
    "- SelectKBest (t-test)\n",
    "- then SVM\n",
    "\n",
    "Compare performance for k = 50, 200, 500.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca418b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "pipe_fs = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('select', SelectKBest(score_func=f_classif, k=200)),\n",
    "    ('svm', SVC(kernel='linear', C=1.0))\n",
    "])\n",
    "\n",
    "cv_scores = cross_val_score(pipe_fs, X_train, y_train, cv=5, scoring='accuracy')\n",
    "print('CV accuracy with feature selection:', cv_scores.mean(), '+/-', cv_scores.std())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
