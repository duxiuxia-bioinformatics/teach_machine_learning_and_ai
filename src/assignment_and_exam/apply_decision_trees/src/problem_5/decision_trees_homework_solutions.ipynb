{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c17f31fd",
   "metadata": {},
   "source": [
    "# Decision Trees Homework — Complete Solution Notebook\n",
    "\n",
    "This notebook provides a complete, reproducible solution for the **Decision Trees** homework:\n",
    "- Hand calculations: entropy, information gain, Gini\n",
    "- Conceptual answers (in markdown)\n",
    "- sklearn coding: training a tree, decision boundary, tree visualization, metrics\n",
    "- ROC probability “chunkiness” + smoothing ideas\n",
    "- Feature importance caveats\n",
    "- Regression tree mini-demo (IC50 style)\n",
    "\n",
    "> **Note:** Your numeric results may differ slightly if you change random seeds or train/test split.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190eaef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification, make_regression\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, confusion_matrix, classification_report,\n",
    "    roc_curve, auc, precision_recall_curve, average_precision_score\n",
    ")\n",
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4d6b64",
   "metadata": {},
   "source": [
    "## Problems 1, 4, 5, 6, 8, 9, 10, 11, 12 — Conceptual Answer Key\n",
    "\n",
    "See the written answer key provided alongside this notebook (same content)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3f28c6",
   "metadata": {},
   "source": [
    "## Problem 2 — Entropy and Information Gain (By hand + verified by code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baed496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset from Problem 2/3\n",
    "data = [\n",
    "    (\"Yes\",\"Yes\",1),\n",
    "    (\"Yes\",\"Yes\",1),\n",
    "    (\"Yes\",\"No\",1),\n",
    "    (\"Yes\",\"No\",0),\n",
    "    (\"No\",\"Yes\",1),\n",
    "    (\"No\",\"Yes\",0),\n",
    "    (\"No\",\"No\",0),\n",
    "    (\"No\",\"No\",0),\n",
    "    (\"No\",\"No\",0),\n",
    "    (\"Yes\",\"Yes\",1),\n",
    "]\n",
    "df = pd.DataFrame(data, columns=[\"Smoking\",\"Mutation\",\"Disease\"])\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb54051",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(y):\n",
    "    y = np.array(y)\n",
    "    n = len(y)\n",
    "    if n == 0:\n",
    "        return 0.0\n",
    "    vals, counts = np.unique(y, return_counts=True)\n",
    "    ps = counts / n\n",
    "    return float(-np.sum([p*np.log2(p) for p in ps if p > 0]))\n",
    "\n",
    "def info_gain(df, feature, target=\"Disease\"):\n",
    "    H = entropy(df[target])\n",
    "    n = len(df)\n",
    "    cond = 0.0\n",
    "    for val, sub in df.groupby(feature):\n",
    "        cond += (len(sub)/n)*entropy(sub[target])\n",
    "    return H - cond\n",
    "\n",
    "H_all = entropy(df[\"Disease\"])\n",
    "IG_S = info_gain(df, \"Smoking\")\n",
    "IG_M = info_gain(df, \"Mutation\")\n",
    "\n",
    "H_all, IG_S, IG_M\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b126709",
   "metadata": {},
   "source": [
    "**Expected outputs (rounded):**\n",
    "- Entropy of full dataset: ~0.971 bits\n",
    "- IG(Smoking): ~0.322 bits\n",
    "- IG(Mutation): ~0.125 bits  \n",
    "So **Smoking** is the best root split (higher information gain)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45c421a",
   "metadata": {},
   "source": [
    "## Problem 3 — Gini Impurity Split (By hand + verified by code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a520ebd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(y):\n",
    "    y = np.array(y)\n",
    "    n = len(y)\n",
    "    if n == 0:\n",
    "        return 0.0\n",
    "    vals, counts = np.unique(y, return_counts=True)\n",
    "    ps = counts / n\n",
    "    return float(1 - np.sum(ps**2))\n",
    "\n",
    "def weighted_gini_after_split(df, feature, target=\"Disease\"):\n",
    "    n = len(df)\n",
    "    wg = 0.0\n",
    "    for val, sub in df.groupby(feature):\n",
    "        wg += (len(sub)/n)*gini(sub[target])\n",
    "    return wg\n",
    "\n",
    "G_all = gini(df[\"Disease\"])\n",
    "WG_S = weighted_gini_after_split(df, \"Smoking\")\n",
    "WG_M = weighted_gini_after_split(df, \"Mutation\")\n",
    "\n",
    "G_all, WG_S, WG_M\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc344de",
   "metadata": {},
   "source": [
    "**Expected outputs (rounded):**\n",
    "- Gini(full): 0.420\n",
    "- Weighted Gini after split on Smoking: ~0.233\n",
    "- Weighted Gini after split on Mutation: 0.400  \n",
    "Lower is better ⇒ **Smoking** is again the best root split. So **Gini agrees with entropy here**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e99a5e9",
   "metadata": {},
   "source": [
    "## Problem 7 — Coding: Train & Visualize a Tree (sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38fb967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 2D dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=300,\n",
    "    n_features=2,\n",
    "    n_informative=2,\n",
    "    n_redundant=0,\n",
    "    n_repeated=0,\n",
    "    n_clusters_per_class=1,\n",
    "    class_sep=1.2,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=0, stratify=y\n",
    ")\n",
    "\n",
    "tree = DecisionTreeClassifier(max_depth=3, random_state=0)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "yhat_train = tree.predict(X_train)\n",
    "yhat_test = tree.predict(X_test)\n",
    "\n",
    "train_acc = accuracy_score(y_train, yhat_train)\n",
    "test_acc = accuracy_score(y_test, yhat_test)\n",
    "cm = confusion_matrix(y_test, yhat_test)\n",
    "\n",
    "train_acc, test_acc, cm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da122fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision boundary plot (2D)\n",
    "def plot_decision_boundary(clf, X, y, title=\"Decision boundary\"):\n",
    "    x_min, x_max = X[:,0].min() - 1.0, X[:,0].max() + 1.0\n",
    "    y_min, y_max = X[:,1].min() - 1.0, X[:,1].max() + 1.0\n",
    "\n",
    "    xx, yy = np.meshgrid(\n",
    "        np.linspace(x_min, x_max, 400),\n",
    "        np.linspace(y_min, y_max, 400)\n",
    "    )\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "    Z = clf.predict(grid).reshape(xx.shape)\n",
    "\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.contourf(xx, yy, Z, alpha=0.25)\n",
    "    plt.scatter(X[:,0], X[:,1], c=y, s=25)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Feature 1\")\n",
    "    plt.ylabel(\"Feature 2\")\n",
    "    plt.show()\n",
    "\n",
    "plot_decision_boundary(tree, X_train, y_train, title=\"Decision boundary (train), max_depth=3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bddb935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the tree\n",
    "plt.figure(figsize=(14,6))\n",
    "plot_tree(tree, filled=True, feature_names=[\"Feature1\",\"Feature2\"], class_names=[\"0\",\"1\"], fontsize=9)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0520bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report metrics\n",
    "print(\"Training accuracy:\", train_acc)\n",
    "print(\"Test accuracy:\", test_acc)\n",
    "print(\"\\nConfusion matrix (test):\\n\", cm)\n",
    "print(\"\\nClassification report (test):\\n\", classification_report(y_test, yhat_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca335831",
   "metadata": {},
   "source": [
    "## Problem 8 — ROC curve and probability 'chunkiness'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313589fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC for the single tree\n",
    "proba_tree = tree.predict_proba(X_test)[:,1]\n",
    "fpr, tpr, thresh = roc_curve(y_test, proba_tree)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.plot(fpr, tpr)\n",
    "plt.plot([0,1],[0,1], linestyle=\"--\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(f\"ROC (Decision Tree) AUC={roc_auc:.3f}\")\n",
    "plt.show()\n",
    "\n",
    "# Show how many distinct probability values appear\n",
    "unique_probs = np.unique(np.round(proba_tree, 6))\n",
    "len(unique_probs), unique_probs[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9ca81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smoother ROC idea 1: Random Forest (averages many trees → more probability levels)\n",
    "rf = RandomForestClassifier(n_estimators=300, random_state=0)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "proba_rf = rf.predict_proba(X_test)[:,1]\n",
    "fpr2, tpr2, _ = roc_curve(y_test, proba_rf)\n",
    "roc_auc2 = auc(fpr2, tpr2)\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.plot(fpr, tpr, label=f\"Tree AUC={roc_auc:.3f}\")\n",
    "plt.plot(fpr2, tpr2, label=f\"RF AUC={roc_auc2:.3f}\")\n",
    "plt.plot([0,1],[0,1], linestyle=\"--\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC: Tree vs Random Forest\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "len(np.unique(np.round(proba_rf, 6)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7f1276",
   "metadata": {},
   "source": [
    "## Problem 11 — Regression Trees (IC50-style) mini demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5124bb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a regression dataset and fit a regression tree\n",
    "Xr, yr = make_regression(n_samples=400, n_features=5, n_informative=3, noise=25.0, random_state=0)\n",
    "Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr, test_size=0.3, random_state=0)\n",
    "\n",
    "rt = DecisionTreeRegressor(max_depth=4, random_state=0)\n",
    "rt.fit(Xr_train, yr_train)\n",
    "\n",
    "pred = rt.predict(Xr_test)\n",
    "\n",
    "# Quick sanity metrics\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "mse = mean_squared_error(yr_test, pred)\n",
    "r2 = r2_score(yr_test, pred)\n",
    "\n",
    "mse, r2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f279ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize regression tree structure\n",
    "plt.figure(figsize=(16,6))\n",
    "plot_tree(rt, filled=True, feature_names=[f\"X{i}\" for i in range(Xr.shape[1])], fontsize=8)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
